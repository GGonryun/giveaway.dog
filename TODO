# Dashboard Backend

### Event model & data schema (instrument everything) Instrument via event tracking (server + client). Strongly prefer event-first design so you can recompute metrics.

- Primary events (name + minimal properties)
  - view_raffle { raffle_id, user_id?, session_id, referrer, utm, ip, ua, timestamp }
  - start_entry { raffle_id, user_id?, session_id, timestamp }
  - submit_entry { raffle_id, user_id?, entry_id, channel (web/form), payload_meta (email present), timestamp }
  - confirm_email { raffle_id, user_id, entry_id, verified, timestamp }
  - social_share { raffle_id, user_id?, platform, share_url, timestamp }
  - disqualified_entry { entry_id, reason (spam, duplicate, invalid_email, geo_restricted), rules_applied, timestamp }
  - flagged_as_bot { entry_id, score, signals: {ip_risk, device_fp, velocity}, timestamp }

- Relational tables (Postgres example)
  - raffles (id, title, start_at, end_at, prize, host_id, config_json)
  - entries (id, raffle_id, user_id, email, status [pending, accepted, disqualified], created_at, device_fp, ip, ua, source)
  - events (id, event_type, payload JSONB, ts) append-only event log
  - disqualifications (entry_id, reason, rule, created_by, created_at)

- Design notes: Keep raw events (unprocessed) in an event store table and then aggregate and build daily materialized aggregates (per raffle/day) for fast dashboard loads.
  Index by (raffle_id, day) and (created_at) for time series. Use a user_candidates table if you allow anonymous entries then convert on signup.

### Aggregation strategy & scaling

- Maintain events + nightly jobs (CRON) to compute daily aggregates into raffle_daily_aggregates. Use indexed queries for smaller time ranges.
- Freshness: Provide two modes: near-real time (last 5m via incremental aggregations) and historical (daily aggregates). Show a "data freshness" timestamp on dashboard.

### Bot/fraud detection approach (how to compute the pie chart)

- Use a multi-layered approach — rules + ML + third-party signals.
- Signals to collect:
- IP velocity & reputation (abuse DBs, repeat IPs).
- Device fingerprinting (canvas, plugins, UA quirks).
- Behavior (time to fill form, mouse/touch patterns, event timings).
- Email hygiene (syntax, domain validity, disposable provider lists).
- Honeypot fields & trap questions (hidden inputs; attention checks).

- Detection stack
  - Realtime rule engine: velocity rules, IP blacklists, failed honeypot → immediate flag/disqualify.
  - Score model: combine signals into a risk score (0–100) and threshold for automatic discard vs manual review.
  - Third-party services (optional): Cloudflare Bot Management, PerimeterX, or dedicated bot detection APIs for high risk clients.

- UI for operators
  - Show list of flagged entries with signals and risk score, allow override and mark as false positive.
  - Show the Bot vs Legit pie and allow toggling to include/exclude flagged entries for other metrics.

# Audit Log Page

- Audit log for any overrides (who, why, timestamp) for admin override of bot flags.
- Include user context (ID, email) and session info (IP, user agent) for each action.
- Keep track of every backend change to detection rules or thresholds.

# Ideas:

- Notifications/Alerts: email / Slack alert on sudden spikes in entries or bot rate > X% in last 5m.
- Rules UI: enable non-dev staff to configure simple rules (rate thresholds, blocklists).

# sweepstakes page

Key findings (research highlights)

- Sweepstakes/giveaway dashboards should emphasize entries, unique entrants, conversion rate, new leads (emails), referrals/shares, and fraud/bot rate.
- Social giveaways produce outsized engagement but follower growth may be temporary — measure lead quality (email hygiene, downstream conversions), not just raw counts.

1. Goals (what the Sweepstakes page must achieve) Give logged-in admin users a single place to browse and manage all sweepstakes they created.

Surface campaign health: entries, conversion, engagement, fraud rate, and trend.
Provide powerful filtering/sorting (date created, status, popularity, bot rate).
Enable quick actions: view details, pause/close, export entries, review flagged submissions.
Be a focused subset of /app/dashboard with sweepstakes-specific visualizations and operational tools.

2. Frontend — UI & Visualizations (priority → wireframe-ready components)
   Tabs: We need to separate all of the different categories into tabs to avoid overwhelming the user. Suggested tabs: (Preview, Analytics, Promote, Entries, Tasks)

   Top: filter bar + primary actions (Create Sweepstakes, Bulk Actions, Export, Live Toggle).

Row 1: KPI cards (Entries today, Unique entrants, Conversion rate, New leads (30d), Avg time-to-entry, Bot rate %).

Row 2: Time-series and funnel (two-column):

Left: Daily entries time-series (stack by source / raffle).

Right: Conversion funnel (visit → start → submit → confirm_email → qualified lead).

Row 3: Lists/Breakdowns (three-column responsive):

Active Sweepstakes list (cards with small chart + KPI badges).

Top Referrers / Channels bar chart.

Bot vs Legit pie (drilldown to reasons).

Row 4: Operational panels (tabs): Latest new users, Flagged entries (manual review), Export & scheduling, Settings & rules editor.

2.2 Components (deliverables for frontend agents)
SweepstakesPage.tsx — server fetches aggregates for selected filters.

FilterBar.tsx — date range, status selector, raffle selector, search, live toggle.

KpiCards.tsx — reusable small KPI card component with delta indicator and tooltip.

TimeSeriesChart.tsx — daily entries (supports stacked series & compare-to-previous).

ConversionFunnel.tsx — funnel with counts and % drop; click to drill into funnel step.

SweepstakesList.tsx — paginated cards with inline mini-sparkline, badges (entries, conversion, bot rate, time left), actions.

BotPie.tsx — interactive pie chart with slices per disqualification reason; click slice → filtered flagged list.

FlaggedEntriesTable.tsx — sortable table with signals (ip, device_fp, time_to_submit, risk_score) and Accept/Deny buttons.

UserPreview.tsx — shows small profile, email verification status, first source, and recent activity.

ExportModal.tsx — choose columns/filters, schedule or immediate CSV/JSON export.

RulesEditor.tsx — configure simple rules (rate limits, blocklists, thresholds).

2.3 UX details & interactions
All charts: hover tooltips with exact numbers + link to filtered view.

Date comparison toggle (compare to previous period).

Live toggle: when ON, small real-time indicator (polling or websocket). Show “data freshness” timestamp.

Inline quick actions: Pause/Resume raffle, Open details, Export, Promote (link to promo modal).

Accessibility: axis labels, aria attributes, keyboard operable tables.

2.4 Mock data & acceptance criteria for UI
Supply mock API with: activeRaffles, raffleDaily, flaggedEntries, topReferrers, newUsers.

Each chart must render correctly with edge cases (no data, large spikes, lots of referrers).

Flagging UI: Accept/Deny updates table locally and invokes backend endpoint; show optimistic state.

4. How the Sweepstakes page relates to /app/dashboard (reuse & consistency)
   Shared KPIs: entries, unique entrants, conversion rate, bot rate, top referrers — reuse the same aggregation endpoints and chart components.

Sweepstakes page = scoped dashboard: the sweepstakes page shows the same visualizations but scoped to a single raffle (or filtered subset) and adds operational tools (flag review, pause, export).

Shared components: KPI cards, time-series chart, bot pie, flagged entries table should be componentized so /app/dashboard and /app/sweepstakes/:id import the same UI.

5. Additional useful functionality (research + product suggestions)
   Referral tracking & viral loops: auto-generate referral links, attribute entries, show leaderboard of top referrers. (common in contest tools).

Lead quality signals: email domain scoring, activity after signup, geolocation — help marketers prioritize leads.

Legal & compliance helpers: auto-generate rules/legal language for regional sweepstakes requirements, age checks, geographic restrictions (many prize platforms include this).

Promotion tools: one-click share cards, UTM templates, scheduled posts.

## Backend

3. Backend — data, APIs, rules, ops (implement after frontend mock is validated)
   3.1 Data model (core tables / event model)
   events (append-only): { id, event_type, payload JSONB, created_at } — captures view_raffle, start_entry, submit_entry, confirm_email, social_share, flagged_as_bot, disqualified_entry.

raffles (id, host_id, title, start_at, end_at, status, config JSONB).

entries (id, raffle_id, user_id?, email_hash, status, device_fp, ip, ua, created_at).

disqualifications (entry_id, reason, rule, details, created_at, created_by).

users (id, email_hash, created_at, verified_at, source).

aggregates (raffle_id, date, entries, unique_entrants, disqualified, bot_count, pageviews, conversions).

3.2 Key backend endpoints (examples)
GET /api/sweepstakes?status=active&sort=popularity&page=1 — returns paginated raffle cards (mini KPIs + sparkline).

GET /api/sweepstakes/:id/metrics?from=&to= — returns time series + funnel + referrers + flagged breakdown.

GET /api/sweepstakes/:id/flagged?limit= — flagged entries with signals.

POST /api/sweepstakes/:id/flagged/:entryId/resolve — accept/deny with audit reason.

POST /api/exports — request CSV/JSON export; returns job id + download when ready.

POST /api/rules (admin) — CRUD for simple rule configurations.

3.3 Aggregation & freshness strategy
Event-first design: store raw events, recompute aggregates.

Materialized daily aggregates for fast reads. Maintain small hourly/near-realtime increments for “live” data (e.g., last 5–15 min).

Small traffic: cron job to recompute daily aggregates from events.

Large traffic: stream events via Kafka → consumer → OLAP (ClickHouse / BigQuery) and push computed aggregates to read DB.

3.4 Bot/fraud detection pipeline
Realtime rules: immediate disqualification for obvious patterns (honeypot hit, disposable email, ip blacklist, velocity thresholds).

Risk scoring: run a lightweight scoring combining signals (time_to_submit, device_fp uniqueness, ip reputation, email hygiene) and store risk_score.

Manual review queue: entries between thresholds go into flagged for human review.

Optionally integrate third-party vendors (Cloudflare Bot Management, PerimeterX) for high-risk campaigns.
open-appsec
KickoffLabs

# users page

2 — Backend (data model, APIs, scoring, exports, syncs)
2.1 Core data model (tables & events)
users (id, email_hash, display_name, region, created_at, verified_at, status [active, blocked, manual_review, trusted], meta JSONB)

entries (id, raffle_id, user_id?, entry_time, ip_hash, device_fp_hash, ua, source, payload JSONB)

events (append-only event log as earlier)

user_metrics (user_id, total_entries, first_entry_at, last_entry_at, avg_time_to_submit, last_active) — computed aggregates

user_quality_scores (user_id, score_float, breakdown JSONB, computed_at)

manual_reviews (id, user_id, entry_id?, reviewer_id, action [accept, deny, flag], reason, created_at)

crm_connectors (org_id, provider, config, last_synced_at, status, error_summary)

sync_jobs (id, connector_id, started_at, finished_at, status, processed_count, error_count, log_url)

exports (id, requested_by, filters_json, status, file_url, created_at)

audit_logs (actor_id, action, target_type, target_id, details, created_at)

2.2 Key backend endpoints (examples)
GET /api/users?query=&page=&perPage=&sort=&filters= → paginated users + metrics.

GET /api/users/:id → user detail + entries + quality score + flags.

POST /api/users/:id/actions → body { action: "block"|"trust"|"tag", reason } → returns audit record.

GET /api/manual_review?status=pending → list of flagged users.

POST /api/manual_review/:id/resolve → accept/deny + reason → updates user status & audit.

POST /api/exports → create export job; returns job_id. GET /api/exports/:id → status + file_url.

GET /api/crm/connectors → connector list & status.

POST /api/crm/connectors/:id/sync → start sync job (returns job_id).

GET /api/crm/sync_jobs/:id → status, processed_count, errors, log_url.

GET /api/users/quality_rules → rules and thresholds used for scoring (for UI transparency).

2.3 Quality scoring pipeline
Signals to collect (minimum):

email_hygiene (valid_format, domain_age, disposable_provider)

ip_reputation / ip_velocity (repeated IPs across entries)

device_fp_uniqueness (how many entries share same fp)

entries_per_user (counts & unusual distribution)

behavior (time_to_fill, event patterns)

verification (email_confirmed, phone_confirmed)

geolocation_consistency (ip vs claimed region)

Scoring model (example weights) — compute 0..100:

email_hygiene: 25%

ip_reputation/velocity: 20%

device_fp_uniqueness: 20%

behavior signals: 15%

verification: 10%

other heuristics (entries_count anomalies): 10%

Compute score = sum(weight_i \* normalized_signal_i); store breakdown JSON for UI.

Define thresholds: >=75 (high), 50–74 (medium), <50 (low). Auto-filter low-quality leads if org setting enabled.

2.4 Syncing to CRMs
Support providers via connector records (oauth or API key) and a connector adapter pattern.

Sync job flow:

POST /api/crm/connectors/:id/sync enqueues job with filters.

Worker pulls job, queries users by filter, transforms fields (mapping config), and calls provider API with rate limiting & retries.

Job metrics update sync_jobs (processed_count, errors).

On error, save detailed logs and surface a summarized error to UI.

Connector features:

Field mapping UI (email → Email, name → FullName, tags → List).

Deduplication: use provider upsert by email or external id.

Webhook handling for connector auth/refresh tokens.

Permissions and per-org connector configs.

2.5 Exports & job queue
Exports must be background jobs writing files to S3 (signed URL). Provide progress and pagination.

For large exports, stream results, and chunk writes to avoid memory spikes.

Keep export retention policy for generated files.

2.6 Search & performance
Use a dedicated search index (Elasticsearch / Postgres pg_trgm + materialized columns) for name/email search across large datasets.

Virtualized frontend + server-side pagination.

Indices on entries.user_id, user_quality_scores.computed_at, users.status, and last_entry_at.

2.7 Security, privacy & compliance
Hash emails (HMAC with org-specific salt) for dedupe while minimizing PII risk; store raw email only when necessary and encrypted.

Provide data deletion endpoints, retention policy, and export of PII on request (GDPR/CCPA).

Limit access to manual-review actions via RBAC. Audit all actions.

Rate-limit CRM connector calls and provide retry/backoff.

3 — Admin flows & trust controls (expanded)
Manual review queue: prioritizes users with score < threshold or score in [50..60] depending on config. Each item includes “why flagged” (signals) and links to related entries. Reviewers must select an action and enter a reason. Action creates manual_reviews and updates user status.

Transparency: in UserDetailPanel, show quality_score.breakdown and the rule set used to compute it. Allow toggling to “show raw signals” (for admins).

Override logs: every accept/deny/block/trust must be auditable with reviewer id and reason.

Auto-mitigation: configurable automation (e.g., auto-block if score < X and device_fp repeated > Y). Admins can enable/disable these.

Appeal/restore flow: ability to restore wrongly blocked users and mark false-positives; stores reason for learning.

4 — API contract examples (JSON shapes)
GET /api/users?page=1&perPage=30&min_score=60
Response:

json
Copy
Edit
{
"items": [
{
"user_id":"u_123",
"display_name":"Alicia M",
"email_masked":"al***@example.com",
"region":"US-CA",
"total_entries": 7,
"first_entry_at":"2025-07-08T12:00:00Z",
"last_entry_at":"2025-08-06T18:22:00Z",
"quality_score": 78,
"score_breakdown": {"email_hygiene": 90, "ip_reputation": 70, "device_fp": 85, "behavior": 60},
"status": "active"
}
],
"page":1,"perPage":30,"total":5240
}
POST /api/manual_review/:id/resolve
Request:

json
Copy
Edit
{ "action":"accept", "reviewer_id":"admin_7", "reason":"Looks human — shared phone verification." }
Response 200:

json
Copy
Edit
{ "ok": true, "manual_review_id": "mr_987", "user_status":"trusted", "audit_id":"audit_128" }
POST /api/crm/connectors/:id/sync
Response:

json
Copy
Edit
{ "job_id":"sync_2025_0001", "status":"queued", "queued_at":"2025-08-10T22:00:00Z" }
5 — Quality score example SQL (materialized)
A simple example to compute an aggregate score daily into user_quality_scores (pseudo-SQL — adapt to your signals):

sql
Copy
Edit
INSERT INTO user*quality_scores (user_id, score_float, breakdown, computed_at)
SELECT
u.id,
(0.25 * email*score +
0.20 * ip*score +
0.20 * device*score +
0.15 * behavior*score +
0.10 * verification*score +
0.10 * anomaly_score
) AS score,
jsonb_build_object(
'email', email_score, 'ip', ip_score, 'device', device_score,
'behavior', behavior_score, 'verification', verification_score,'anomaly', anomaly_score
) AS breakdown,
now()
FROM users u
JOIN user_aggregate_signals s ON s.user_id = u.id
WHERE u.org_id = :org_id;
Run via daily worker; flag and enqueue those under threshold for manual review.

6 — Acceptance criteria (what “done” looks like)
Users page UI renders with paginated users and search; filters and sorts update results.

Each user row shows quality score and a tooltip with breakdown.

Clicking a user opens detail panel with entries history, signals, and action buttons.

Manual review queue shows flagged users and supports accept/deny with required reason and creates an audit entry.

Export and CRM Sync flows can be triggered; UI shows progress and last_synced_at. If connector absent, prompts to connect.

Backend endpoints respond with correct mock payloads; exports & syncs are background jobs with logs.

7 — Prioritized task list for AI agents (UI-first)
Create design tokens, UsersTable, FilterBar, UserDetailPanel, ManualReviewQueue components.

Implement mock API and realistic test dataset including flagged users and various score distributions.

Wire UI to mock API: search, filters, user detail open, manual review actions (mock POST).

Implement ExportModal & mock export job flow.

Implement CRMSyncModal with Connect flow stub and mock Sync Now progress.

QA: heavy lists, keyboard navigation, no-data states, and role-based UI mocks (admin vs viewer).

Provide API contract / OpenAPI stub for backend team.

8 — Handoff artifacts to prepare for backend agents
OpenAPI spec for endpoints listed above.

Event schema for submit_entry, confirm_email, and user_action events.

SQL definitions for user_metrics, user_quality_scores, and indexes used by searches.

Worker spec for exports & CRM syncs (retry, rate-limit, chunking).

RBAC matrix and audit logging contract.

9 — Suggestions & further product ideas
Allow saving audience segments (high-quality leads, region X, referred by influencer Y) and reusing them for exports or CRM syncs.

Add lead tagging and quick bulk actions for marketing workflows.

Create a “Privacy & Compliance” panel per org showing PII retention and delete requests.

Add scheduled email digests of high-quality new leads to marketing owners.

If you want, I can next:

generate the OpenAPI spec for the endpoints above, or

produce React + Next.js starter components for the Users page against mock APIs, or

produce the SQL and a background worker outline for the quality scoring and sync jobs.

Which of those should I produce next for your AI agents?

### Admin flows & trust controls

- Manual review queue for flagged entries (allow accept/deny + reason).
- Transparency: show “why this entry was flagged” on entry detail (helps client trust).

# system page

we need a public page in the marketing section where any user can view system status updates and any ongoing maintenance or outages.

# roadmap page

we need a public page in the marketing section where any user can view our product roadmap and upcoming features. i really like how posthog's home page does this https://posthog.com/roadmap, allow users to vote for features if possible and to show how many votes they have.
